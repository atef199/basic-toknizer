# Custom Tokenizer Pipeline in Python

This repository contains a Jupyter Notebook (`encoding.ipynb`) that walks through building a **basic tokenizer pipeline** from scratch using Python. The notebook covers each step in detail, making it suitable for both learning and prototyping purposes.

## ðŸ“˜ Features

The tokenizer pipeline includes:

- **Normalization**: Lowercasing, removing unwanted characters, etc.
- **Pre-tokenization**: Splitting text into words or sub-words
- **Tokenization**: Assigning tokens from a vocabulary
- **Post-tokenization**: Optional cleanup or additional rules
- **Numericalization**: Converting tokens to IDs and back